{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Manage Missing Values, Outliers, Normalize, and Transform Data**"
      ],
      "metadata": {
        "id": "nRJ_83O5xJ1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "In machine learning, the quality of your data directly impacts the effectiveness of your models. This makes data preprocessing a critical step in any AI/ML pipeline. Here I’ll cover four key aspects of data preprocessing: `handling missing values`, `managing outliers`, `normalization`, and `transformation`. I will teach you understand and apply these techniques to ensure that your data is clean, consistent, and ready for analysis."
      ],
      "metadata": {
        "id": "Rl97LJVkxQEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Handle missing values**\n",
        "Missing values are a common issue in datasets and can arise for various reasons, such as data entry errors or unavailability of certain information. If not addressed, missing values can lead to biased results or reduce the accuracy of your model.\n",
        "\n",
        "**Strategies for handling missing values:**"
      ],
      "metadata": {
        "id": "8XRIfI0tx0nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Remove missing data:**\n",
        "\n",
        "      **Description:** If a small number of rows or columns have missing values, you might consider removing them from the dataset.\n",
        "\n",
        "      **When to use:** This approach is suitable when the missing data is minimal and its removal won’t significantly impact the dataset.\n",
        "\n",
        "      **Code example:**\n",
        "      ```\n",
        "      # Drop rows with missing values\n",
        "      df_cleaned = df.dropna()\n",
        "\n",
        "      # Drop columns with missing values\n",
        "      df_cleaned = df.dropna(axis=1)\n",
        "      ```\n",
        "2.  **Impute missing data**\n",
        "      **Description:** Imputation involves filling in missing values with a substitute value, such as the mean, median, or mode of the column.\n",
        "\n",
        "     ** When to use:** This is useful when missing data is more prevalent, but you don’t want to lose information by removing rows or columns.\n",
        "\n",
        "      **Code example**\n",
        "      ```\n",
        "      # Fill missing values with the mean of the column\n",
        "      df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
        "\n",
        "      # Fill missing values with the median of the column\n",
        "      df['column_name'].fillna(df['column_name'].median(), inplace=True)\n",
        "      ```\n",
        "      **Note**: I personally do not recommend this technique specially when you are working with the customer's data. You need to clarify with them first!\n",
        "\n",
        "3. **Forward or backward fill**\n",
        "      **Description:** Forward fill propagates the last valid observation forward, while backward fill does the opposite.\n",
        "\n",
        "      **When to use:** This is particularly useful in time series data where trends or sequences are important.\n",
        "\n",
        "      **Code example**\n",
        "    ```\n",
        "    # Forward fill\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # Backward fill\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "    ```\n",
        "\n"
      ],
      "metadata": {
        "id": "E9NrJyrxywaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Manage outliers**\n",
        "Outliers are data points that differ significantly from other observations. They can distort statistical analyses and negatively impact the performance of machine learning models.\n",
        "\n",
        "**Strategies for managing outliers:**"
      ],
      "metadata": {
        "id": "XuYTDBRj0tFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Identify outliers**\n",
        "\n",
        "**Description:** The first step is to identify outliers, which can be done using statistical methods such as Z-score or the Interquartile Range (IQR).\n",
        "\n",
        "**Code example**\n",
        "```\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# Using Z-score to identify outliers\n",
        "z_scores = np.abs(stats.zscore(df['column_name']))\n",
        "outliers = df[z_scores > 3]\n",
        "\n",
        "# Using IQR to identify outliers\n",
        "Q1 = df['column_name'].quantile(0.25)\n",
        "Q3 = df['column_name'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = df[(df['column_name'] < (Q1 - 1.5 * IQR)) | (df['column_name'] > (Q3 + 1.5 * IQR))]\n",
        "```\n",
        "\n",
        "**2. Handle outliers**\n",
        "\n",
        "*   **a. Remove outliers**\n",
        "\n",
        "      **Description:** Outliers can be removed from the dataset if they are believed to be errors or not representative of the population.\n",
        "\n",
        "      **Code example**\n",
        "        ```\n",
        "        # Remove outliers identified by Z-score\n",
        "        df_cleaned = df[(z_scores <= 3)]\n",
        "\n",
        "        # Remove outliers identified by IQR\n",
        "        df_cleaned = df[~((df['column_name'] < (Q1 - 1.5 * IQR)) | (df['column_name'] > (Q3 + 1.5 * IQR)))]\n",
        "        ```\n",
        "\n",
        "*   **b. Cap or transform outliers**\n",
        "\n",
        "     **Description:** Instead of removing outliers, you might cap them to a certain threshold or transform them using logarithmic or other functions to reduce their impact.\n",
        "\n",
        "     **Code example:**\n",
        "\n",
        "      ```\n",
        "      # Cap outliers to a threshold\n",
        "      df['column_name'] = np.where(df['column_name'] > upper_threshold, upper_threshold, df['column_name'])\n",
        "\n",
        "      # Log transform to reduce the impact of outliers\n",
        "      df['column_name_log'] = np.log(df['column_name'] + 1)\n",
        "      ```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o9Jnz65E1hSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normalization**\n",
        "Normalization (or scaling) is the process of adjusting the values of numeric columns in a dataset to a common scale, typically between zero and one. This is especially important for machine learning algorithms that rely on the magnitude of features such as gradient descent-based algorithms.\n",
        "\n",
        "**Methods of normalization:**"
      ],
      "metadata": {
        "id": "IEvSjWGsOM6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Min-Max scaling**\n",
        "\n",
        "  **Description:** Scales all numeric values in a column to a range between zero and one.\n",
        "\n",
        "  **Code example**\n",
        "\n",
        "  ```\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  df['scaled_column'] = scaler.fit_transform(df[['column_name']])\n",
        "  ```\n",
        "\n",
        "**2. Z-score standardization**\n",
        "\n",
        "  **Description:** Scales the data so that it has a mean of zero and a standard deviation of one. This method is useful when you want to compare features with different units or scales.\n",
        "\n",
        "  ```\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  df['standardized_column'] = scaler.fit_transform(df[['column_name']])\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "J-oDyV_OOjR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformation**\n",
        "Data transformation involves converting data from one format or structure to another. This is often necessary to meet the assumptions of statistical models or to improve the performance of machine learning algorithms.\n",
        "\n",
        "**Common data transformations:**"
      ],
      "metadata": {
        "id": "pzRXkWXGPYKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Logarithmic transformation**\n",
        "\n",
        "**Description:** Log transformation is used to stabilize variance, by making the data appear more like normal distribution and reducing the impact of outliers.\n",
        "\n",
        "**Code example:**\n",
        "  ```\n",
        "  df['log_column'] = np.log(df['column_name'] + 1)  # Adding 1 to avoid log(0)\n",
        "```\n",
        "\n",
        "**2. Box-Cox transformation**\n",
        "\n",
        "**Description:** this transformation is used to stabilize variance and make the data more normally distributed.\n",
        "\n",
        "**Code example:**\n",
        "\n",
        "  ```\n",
        "  from scipy import stats\n",
        "  df['boxcox_column'], _ = stats.boxcox(df['column_name'] + 1)  # Adding 1 to avoid log(0)\n",
        "  ```\n",
        "\n",
        "**3. Binning**\n",
        "\n",
        "**Description:** Binning, or discretization, involves converting continuous variables into discrete categories.\n",
        "\n",
        "**Code example**\n",
        "\n",
        "  ```\n",
        "  # Create bins for a continuous variable\n",
        "  df['binned_column'] = pd.cut(df['column_name'], bins=[0, 10, 20, 30], labels=['Low', 'Medium', 'High'])\n",
        "  ```\n",
        "\n",
        "**4. Encoding categorical variables**\n",
        "\n",
        "**Description:** Transforming categorical data into numerical format, which is necessary for many machine learning algorithms.\n",
        "\n",
        "**Code example**\n",
        "\n",
        "  ```\n",
        "  # One-hot encoding\n",
        "  df_encoded = pd.get_dummies(df, columns=['category_column'])\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mxEQ-432jKQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "Handling missing values, managing outliers, normalization, and transformation are essential steps in preparing your data for machine learning. Properly applying these techniques ensures that your dataset is clean, consistent, and in the right format for analysis, leading to more accurate and reliable models.\n",
        "\n",
        "As you work with different datasets, practice these techniques to become proficient in data preprocessing, which is a critical skill in the data science workflow.\n",
        "\n",
        "By mastering these preprocessing techniques, you’ll be better equipped to tackle a wide range of data challenges, ensuring that your models are built on a solid foundation of high-quality data.\n",
        "\n"
      ],
      "metadata": {
        "id": "5N-NyTMDldHK"
      }
    }
  ]
}