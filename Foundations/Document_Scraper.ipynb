{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **A Basic Document Scraper in Python**"
      ],
      "metadata": {
        "id": "69XGslrSemPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "Now that you've set up a basic web scraper using Python, we'll extend that knowledge to fetch a specific document, such as a PDF file or a text file, from a website. This is a common requirement when dealing with web scraping, as many useful data sources are available in downloadable documents."
      ],
      "metadata": {
        "id": "aiIiiqLXe3u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 0: Identify the document link**\n",
        "Before you can download a document, you need to identify its location on the webpage. Typically, documents are linked via ```<a> (anchor)```\n",
        "tags, which you can locate by inspecting the webpage's HTML structure. Something like this:\n",
        "\n",
        "```\n",
        "<a href=\"/files/report.pdf\" class=\"download-link\">Download Report</a>\n",
        "```\n",
        "In this example, the document (report.pdf) is linked within an ```<a>``` tag with the class \"download-link.\" The href attribute contains the path to the document.\n",
        "\n"
      ],
      "metadata": {
        "id": "d7k376wBVRki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Import the necessary libraries**"
      ],
      "metadata": {
        "id": "E0pvgaLXbN4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OhZi0WzSa5_V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Send an HTTP request to the website**\n",
        "Use the requests library to send an HTTP GET request to the website you want to scrape:"
      ],
      "metadata": {
        "id": "uzDr42wVbcln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.who.int/data/sets/global-excess-deaths-associated-with-covid-19-modelled-estimates\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    print(\"Connection was successful!\")\n",
        "else:\n",
        "    print('Failed to retrieve the webpage.')\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gh7Gv7ha8yF",
        "outputId": "64df6360-6f5a-4d00-864c-4651c4940b8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection was successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Parse the HTML content**\n",
        "Once youâ€™ve successfully retrieved the web page, use BeautifulSoup to parse the HTML content and find the ```<a> Tag```.\n",
        "\n"
      ],
      "metadata": {
        "id": "2XzF6-b1c5IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_link = soup.find('a', string=\"Download data\")['href']\n",
        "print('Document link found:', document_link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoqKeLmtcU-F",
        "outputId": "60a04e85-eaae-4e20-cee6-0292a896d0eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document link found: https://cdn.who.int/media/docs/default-source/world-health-data-platform/covid-19-excessmortality/2023-05-19_covid-19_gem.zip?sfvrsn=9a95fc1a_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sends a request to the webpage and parses the HTML. It then searches for an ```<a> tag``` with the class download-link and extracts the href attribute, which contains the path to the document."
      ],
      "metadata": {
        "id": "Gx6m0XnfZCsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Handle relative URLs**\n",
        "The link extracted from the webpage might be a relative URL (e.g., ```/files/report.pdf```) rather than a full URL. You need to convert this into a full URL before making a request to download the document. In that case we need to convert the relative URL to a full URL. Don't fortget to import ```os```. The ```os.path.join()``` function combines the base URL with the relative URL to form a full URL that can be used to download the document."
      ],
      "metadata": {
        "id": "Z8YaitUjZVxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://example.com'\n",
        "relative_document_link = 'files/reports.pdf'\n",
        "full_url = os.path.join(base_url, relative_document_link)\n",
        "print('Full URL:', full_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-TWgO7AZUrI",
        "outputId": "de735f42-02d7-4f18-d508-4feeb265cbc5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full URL: https://example.com/files/reports.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5: Download the document**\n",
        "With the full URL in hand, you can now send a request to download the document. The downloaded file can be saved to your local machine."
      ],
      "metadata": {
        "id": "89K3G5doeYW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_response = requests.get(document_link)\n",
        "\n",
        "if document_response.status_code == 200:\n",
        "  with open('2023-05-19_covid-19_gem.zip', 'wb') as file:\n",
        "    file.write(document_response.content)\n",
        "    print('Document downloaded successfully.')\n",
        "else:\n",
        "  print('Failed to download the document. Status code:', document_response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjAYvXhQeco9",
        "outputId": "e3c1744d-caa7-41eb-9631-f660c25c1405"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Fetch multiple documents**\n",
        "What if the web page has multiple documents and you want to download the all in once.In that case you can modify the scraper to loop through all the document links and download each one. Here, I change the code a bit more and made it a bit advance. For example, you don't need to check the reponse status with the code `200`. You can use the function `raise_for_status()` instead.\n",
        "\n",
        "Moreover, you want to download different file format such as, `pdf`, `zip`, `csv`, `txt`, or `excel`."
      ],
      "metadata": {
        "id": "9wfBB_WhhJNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the connection\n",
        "url = \"https://opendata.dwd.de/climate_environment/health/historical_alerts/heat_warnings/\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Creat BeautifulSoup Object to parse the html\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "links = soup.find_all('a')\n",
        "\n",
        "# This part is optional, but most of the time there is a link which is not a file\n",
        "document_links = [link['href'] for link in links if link['href'] != \"../\"]\n",
        "\n",
        "# If you like to store all the files under a specific folder:\n",
        "download_folder = \"downloaded_files\"\n",
        "os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "allowed_extensions = [\".pdf\", \".zip\", \".docx\", \".xlsx\", \".csv\"]\n",
        "\n",
        "for link in document_links:\n",
        "\n",
        "  # This is for the relative URLs situation\n",
        "  if not link.startswith(\"http\"):\n",
        "    file_url = os.path.join(url, link)\n",
        "\n",
        "  filename = os.path.basename(file_url)\n",
        "  # Now we check if the file format is eligible\n",
        "  if any(filename.lower().endswith(ext) for ext in allowed_extensions):\n",
        "    print(f\"Downloading: {filename}\")\n",
        "\n",
        "    # Stream download\n",
        "    file_response = requests.get(file_url, stream=True)\n",
        "    file_response.raise_for_status()\n",
        "\n",
        "    # Downloading the files\n",
        "    file_path = os.path.join(download_folder, filename)\n",
        "    with open(file_path, \"wb\") as file:\n",
        "      file.write(file_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWCOaAKsh1bl",
        "outputId": "456f3fee-5a83-4526-e793-651f895b2b82"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: Beschreibung_historische-Hitzewarnungen.pdf\n",
            "Downloading: Description_historical_heat_alerts.pdf\n",
            "Downloading: heat_alerts_2005.csv\n",
            "Downloading: heat_alerts_2006.csv\n",
            "Downloading: heat_alerts_2007.csv\n",
            "Downloading: heat_alerts_2008.csv\n",
            "Downloading: heat_alerts_2009.csv\n",
            "Downloading: heat_alerts_2010.csv\n",
            "Downloading: heat_alerts_2011.csv\n",
            "Downloading: heat_alerts_2012.csv\n",
            "Downloading: heat_alerts_2013.csv\n",
            "Downloading: heat_alerts_2014.csv\n",
            "Downloading: heat_alerts_2015.csv\n",
            "Downloading: heat_alerts_2016.csv\n",
            "Downloading: heat_alerts_2017.csv\n",
            "Downloading: heat_alerts_2018.csv\n",
            "Downloading: heat_alerts_2019.csv\n",
            "Downloading: heat_alerts_2020.csv\n",
            "Downloading: heat_alerts_2021.csv\n",
            "Downloading: heat_alerts_2022.csv\n",
            "Downloading: heat_alerts_2023.csv\n",
            "Downloading: heat_alerts_2024.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Important considerations**\n",
        "\n",
        "**Respect website permissions:** Always check the websiteâ€™s terms of service to ensure youâ€™re allowed to download documents using automated tools.\n",
        "\n",
        "**Handle different file types accordingly:** Depending on the type of document, you might need to adjust your code to handle different file formats (e.g., .txt, .csv, and .docx).\n",
        "\n",
        "**Manage large downloads:** If youâ€™re downloading large files, consider adding error handling and resuming capabilities to your scraper."
      ],
      "metadata": {
        "id": "sh-zs_hznsA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        "By extending your web scraper to fetch documents, you can automate the process of acquiring valuable resources from the web. Whether youâ€™re downloading reports, datasets, or other types of files, understanding how to identify document links and handle file downloads is a critical skill in data acquisition. With the examples provided, you should be well equipped to implement document fetching in your web scraping projects.\n",
        "\n",
        "Continue experimenting with different websites and document types to refine your scraper, and always remember to scrape responsibly and ethically."
      ],
      "metadata": {
        "id": "pS_uwEhJn--s"
      }
    }
  ]
}